<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Introduction to Transformers | Mrinal Mathur </title> <meta name="author" content="Mrinal Mathur"> <meta name="description" content="A beginner's guide to the transformer architecture, implementation, and applications"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mrinal18.github.io/blog/2024/distill/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "An Introduction to Transformers",
            "description": "A beginner's guide to the transformer architecture, implementation, and applications",
            "published": "March 15, 2024",
            "authors": [
              
              {
                "author": "Mrinal Mathur",
                "authorURL": "https://www.linkedin.com/in/mmathur4/",
                "affiliations": [
                  {
                    "name": "Amazon",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mrinal</span> Mathur </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Mrinal Mathur </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/dropdown/">submenus </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>An Introduction to Transformers</h1> <p>A beginner's guide to the transformer architecture, implementation, and applications</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#what-are-transformers">What are Transformers?</a> </div> <div> <a href="#transformer-architecture-deep-dive">Transformer Architecture Deep Dive</a> </div> <ul> <li> <a href="#encoder-and-decoder-stacks">Encoder and Decoder Stacks</a> </li> <li> <a href="#multi-head-attention">Multi-Head Attention</a> </li> <li> <a href="#positional-encoding">Positional Encoding</a> </li> <li> <a href="#feed-forward-layers">Feed Forward Layers</a> </li> <li> <a href="#residual-connections-and-layer-normalization">Residual Connections and Layer Normalization</a> </li> </ul> <div> <a href="#implementing-transformers-from-scratch">Implementing Transformers from Scratch</a> </div> <div> <a href="#training-transformers">Training Transformers</a> </div> <div> <a href="#applications-of-transformers">Applications of Transformers</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#references-and-resources">References and Resources</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Transformers have revolutionized the field of natural language processing (NLP) and have become the go-to architecture for many AI applications. This blog post provides a beginner-friendly introduction to transformers, covering their architecture, implementation, and real-world applications. We will dive deep into the key components of transformers and provide code snippets to help you understand how they work under the hood.</p> <h2 id="what-are-transformers">What are Transformers?</h2> <p>Transformers are a type of deep learning model that was introduced in the seminal paper “Attention is All You Need”<d-cite key="vaswani2017attention"></d-cite>. They have an encoder-decoder structure and rely heavily on the attention mechanism to capture dependencies between input and output sequences. Transformers have largely replaced previous approaches like recurrent neural networks (RNNs) and convolutional neural networks (CNNs) for many NLP tasks.</p> <h2 id="transformer-architecture-deep-dive">Transformer Architecture Deep Dive</h2> <p>Let’s take a closer look at the various components that make up the transformer architecture.</p> <h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3> <p>The transformer consists of an encoder and a decoder, each made up of multiple layers. The encoder takes the input sequence and generates a contextualized representation, while the decoder attends to the encoder’s output and generates the output sequence.</p> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>At the heart of the transformer is the multi-head attention mechanism. It allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships and dependencies. Multi-head attention is computed as follows:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O\] <p>where $\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$ and $W^Q_i$, $W^K_i$, $W^V_i$, and $W^O$ are learned projection matrices.</p> <h3 id="positional-encoding">Positional Encoding</h3> <p>Since transformers do not have any inherent notion of position, positional encodings are added to the input embeddings to inject sequence order information. The positional encodings are computed using sine and cosine functions of different frequencies:</p> <p>\(PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})\) \(PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})\)</p> <p>where $pos$ is the position, $i$ is the dimension, and $d_{model}$ is the embedding dimension.</p> <h3 id="feed-forward-layers">Feed Forward Layers</h3> <p>In addition to the attention layers, transformers also include feed forward layers that apply non-linear transformations to the attended representations. These layers typically consist of two linear transformations with a ReLU activation in between:</p> \[\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\] <h3 id="residual-connections-and-layer-normalization">Residual Connections and Layer Normalization</h3> <p>To facilitate training and improve gradient flow, transformers employ residual connections<d-cite key="he2016deep"></d-cite> and layer normalization<d-cite key="ba2016layer"></d-cite>. Residual connections allow the model to learn incremental updates, while layer normalization helps stabilize the activations and gradients.</p> <h2 id="implementing-transformers-from-scratch">Implementing Transformers from Scratch</h2> <p>To gain a deeper understanding of transformers, let’s implement some of the key components from scratch using PyTorch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">q_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_linear</span><span class="p">(</span><span class="n">q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_linear</span><span class="p">(</span><span class="n">k</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_linear</span><span class="p">(</span><span class="n">v</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_linear</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">attn_output</span>
</code></pre></div></div> <p>This code snippet shows how to implement the multi-head attention module in PyTorch. It takes the query, key, and value matrices as input, applies linear projections, computes the attention scores and weights, and returns the attended output.</p> <h2 id="training-transformers">Training Transformers</h2> <p>Training transformers involves preparing the data, setting up the model and optimizer, and running the training loop. Here are some tips for effective training:</p> <ul> <li>Use a large and diverse dataset to capture various linguistic patterns and structures.</li> <li>Employ techniques like data augmentation, dropout, and weight decay to improve generalization.</li> <li>Monitor the training progress using metrics like perplexity and validation loss.</li> <li>Experiment with different hyperparameters, such as learning rate, batch size, and number of layers.</li> <li>Utilize pre-trained models and fine-tuning for faster convergence and better performance.</li> </ul> <h2 id="applications-of-transformers">Applications of Transformers</h2> <p>Transformers have found widespread adoption across various domains and applications. Some notable examples include:</p> <ul> <li>BERT<d-cite key="devlin2019bert"></d-cite>: A pre-trained transformer model for language understanding tasks like sentiment analysis, question answering, and named entity recognition.</li> <li>GPT<d-cite key="radford2018improving"></d-cite>: A transformer-based language model that can generate human-like text and has been used for tasks like dialogue generation and story completion.</li> <li>Vision Transformer (ViT)<d-cite key="dosovitskiy2021image"></d-cite>: An adaptation of transformers for computer vision tasks, achieving state-of-the-art performance on image classification benchmarks.</li> </ul> <p>Transformers have also been applied to various domain-specific tasks, such as protein structure prediction, drug discovery, and code generation, demonstrating their versatility and potential impact.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we provided a comprehensive introduction to transformers, covering their architecture, implementation, training, and applications. We explored the key components of transformers, including multi-head attention, positional encoding, and feed forward layers, and provided code snippets to illustrate their implementation.</p> <p>Transformers have revolutionized the field of NLP and continue to inspire new research and applications. As you dive deeper into the world of transformers, keep an eye out for the latest advancements and techniques to further improve their performance and efficiency.</p> <h2 id="references-and-resources">References and Resources</h2> <d-cite key="vaswani2017attention,he2016deep,ba2016layer,devlin2019bert,radford2018improving,dosovitskiy2021image"></d-cite> <p>For further reading and exploration, check out the following resources:</p> <ul> <li><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention is All You Need</a></li> <li><a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">The Illustrated Transformer</a></li> <li><a href="https://e2eml.school/transformers.html" rel="external nofollow noopener" target="_blank">Transformers from Scratch</a></li> <li><a href="https://huggingface.co/transformers/" rel="external nofollow noopener" target="_blank">Hugging Face Transformers Library</a></li> </ul> <p>Happy learning and experimenting with transformers!</p> <p>Citations: [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/12724226/f9f6fbd5-e6f4-4664-a668-92333b7952d4/2018-12-22-distill.md</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"mrinal18/mrinal18.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Mrinal Mathur. Last updated: March 15, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>